üìä VPC Flow Logs Egress Analyzer

VPC Flow Logs Egress Analyzer is a lightweight, one-shot Go tool designed to:
‚Ä¢	Fetch AWS VPC Flow Logs directly from S3
‚Ä¢	Parse and classify traffic as egress, ingress, or internal
‚Ä¢	Aggregate data per destination IP (only for egress)
‚Ä¢	Calculate the AWS NAT Gateway Data Processed cost based on region
‚Ä¢	Export a clean, sorted summary as result.json
‚Ä¢	Display a human-readable summary in the console
‚Ä¢	Use a local cache to avoid re-downloading and re-parsing logs

This tool is ideal for identifying:
‚Ä¢	Unexpected outbound traffic
‚Ä¢	NAT Gateway cost leaks
‚Ä¢	External services your infrastructure is communicating with
‚Ä¢	Daily egress volume per IP


‚∏ª

‚ú® Features

‚úî Retrieve VPC Flow Logs directly from S3

Uses your AWS Account ID, region, and bucket structure:

```
AWSLogs/<ACCOUNT_ID>/vpcflowlogs/<region>/<year>/<month>/<day>/
```

‚úî Full VPC Flow Log parser (AWS format V2)

‚úî Egress/Internal/Ingress detection

Automatic classification based on IP ranges.

‚úî Per-IP aggregation

For each external IP:
‚Ä¢	total bytes
‚Ä¢	GB processed
‚Ä¢	NAT cost (region-aware)
‚Ä¢	connection count

‚úî NAT Gateway cost estimation

Uses official AWS NAT Gateway Data Processed pricing, region-specific.

‚úî Smart caching

Downloaded logs are written to .cache/YYYY-MM-DD.json.gz
‚Üí next runs are instant.

‚úî Clean final output
‚Ä¢	result.json is sorted by GB DESC
‚Ä¢	Console summary is compact and readable

‚úî AWS credentials auto-loaded

No need to hardcode credentials, see below.

‚∏ª

‚öôÔ∏è Environment Variables

The following variables configure the tool.
They are automatically loaded with defaults using:

```
func DefaultEnvValues() map[string]string {
	now := time.Now()
	year, month, day := now.Date()

	return map[string]string{
		"AWS_REGION":            "eu-west-3",
		"AWS_ACCESS_KEY_ID":     "",
		"AWS_SECRET_ACCESS_KEY": "",
		"S3_BUCKET_NAME":        "",
		"S3_PREFIX":             "",
		"AWS_ACCOUNT_ID":        "",
		"YEAR":                  fmt.Sprintf("%04d", year),
		"MONTH":                 fmt.Sprintf("%02d", int(month)),
		"DAY":                   fmt.Sprintf("%02d", day),
	}
}
```

| Variable               | Description                                      | Default Value      |
|------------------------|--------------------------------------------------|--------------------|
| AWS_REGION             | AWS region where the VPC Flow Logs are stored    | eu-west-3          |
| AWS_ACCESS_KEY_ID      | Your AWS Access Key ID                           | (empty)             |
| AWS_SECRET_ACCESS_KEY  | Your AWS Secret Access Key                       | (empty)             |
| S3_BUCKET_NAME         | Name of the S3 bucket containing the Flow Logs   | (empty)             |
| S3_PREFIX              | Prefix path in the S3 bucket                        | (empty)             |
| AWS_ACCOUNT_ID         | Your AWS Account ID                              | (empty)             |
| YEAR                   | Year of the logs to fetch (YYYY)                 | Current year       |
| MONTH                  | Month of the logs to fetch (MM)                  | Current month      |
| DAY                    | Day of the logs to fetch (DD)                    | Current day        |

‚∏ª

üîê AWS Authentication

You do not need to provide AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY if:

‚úî You run using Docker and mount your local AWS credentials:

```
-v ~/.aws:/root/.aws:ro
```


‚úî You are authenticated via:
‚Ä¢	AWS CLI v2 SSO
‚Ä¢	MFA sessions
‚Ä¢	Named profiles
‚Ä¢	~/.aws/credentials or ~/.aws/config

The tool will automatically pick up:
‚Ä¢	your default profile
‚Ä¢	or whatever profile is configured in your shell

No credentials are ever stored by the tool.

‚∏ª

üóÑ Caching Behavior

The tool creates a .cache/ folder (gitignored):

Cache contains:
‚Ä¢	fully parsed VPC Flow Logs
‚Ä¢	enriched records (direction, GB, cost, etc.)

If a cache file already exists for the selected date:
‚Üí S3 is not queried
‚Üí Logs are loaded instantly from disk

You can safely delete .cache/ at any time.

‚∏ª

üì¶ Output: result.json

A clean JSON file is generated:
‚Ä¢	sorted by GB descending
‚Ä¢	including full cost breakdown
‚Ä¢	per-IP details
‚Ä¢	total cost summary

Example result.json:

```json
{
  "year": "2025",
  "month": "11",
  "day": "30",
  "region": "eu-west-3",
  "cost_per_gb_usd": 0.062,
  "total": {
    "bytes": 5772308394,
    "gb": 5.382,
    "cost_usd": 0.3337
  },
  "egress_by_ip": [
    {
      "ip": "3.5.204.12",
      "direction": "egress",
      "bytes": 1090519040,
      "gb": 1.015625,
      "cost_usd": 0.063,
      "connection_count": 120
    }
  ]
}
```

‚∏ª

üê≥ Running with Docker

The Makefile handles everything.

Build the image:

```
make build docker
```

Run analysis:
```
make run docker
```


